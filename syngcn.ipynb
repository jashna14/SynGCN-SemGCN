{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade10bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Model\n",
    "from helper import *\n",
    "import tensorflow as tf, time, ctypes\n",
    "from sparse import COO\n",
    "from web.embedding import Embedding\n",
    "from web.evaluate  import evaluate_on_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4094b359",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SynGCN(Model):\n",
    "\n",
    "\tdef getBatches(self, shuffle = True):\n",
    "\t\tself.lib.reset()\n",
    "\t\twhile True:\n",
    "\t\t\t# max_len = 0; unused variable\n",
    "\t\t\teph_ovr = self.lib.getBatch(self.edges_addr, self.wrds_addr, self.negs_addr, self.samp_addr, self.elen_addr, self.wlen_addr, \n",
    "\t\t\t\t\t  \t    self.p.win_size, self.p.num_neg, self.p.batch_size, ctypes.c_float(self.p.sample))\n",
    "\t\t\tif eph_ovr == 1: break\n",
    "\t\t\tyield {'edges': self.edges, 'wrds': self.wrds, 'negs': self.negs, 'sample': self.samp, 'elen': self.elen, 'wlen': self.wlen}\n",
    "\n",
    "\tdef load_data(self):\n",
    "\t\tself.logger.info(\"Loading data\")\n",
    "\n",
    "\t\tself.voc2id         = read_mappings('./data/voc2id.txt');   self.voc2id  = {k:      int(v) for k, v in self.voc2id.items()}\n",
    "\t\tself.id2freq        = read_mappings('./data/id2freq.txt');  self.id2freq = {int(k): int(v) for k, v in self.id2freq.items()}\n",
    "\t\tself.id2voc \t    = {v:k for k, v in self.voc2id.items()}\n",
    "\t\tself.vocab_size     = len(self.voc2id)\n",
    "\t\tself.wrd_list\t    = [self.id2voc[i] for i in range(self.vocab_size)]\n",
    "\n",
    "\t\tself.de2id\t    = read_mappings('./data/de2id.txt');    self.de2id   = {k:      int(v) for k, v in self.de2id.items()}\n",
    "\t\tself.num_deLabel    = len(self.de2id)\n",
    "\n",
    "\t\t# Calculating rejection probability\n",
    "\t\tcorpus_size    \t    = np.sum(list(self.id2freq.values()))\n",
    "\t\trel_freq \t    = {_id: freq/corpus_size for _id, freq in self.id2freq.items()}\n",
    "\t\tself.rej_prob \t    = {_id: (1-self.p.sample/rel_freq[_id])-np.sqrt(self.p.sample/rel_freq[_id]) for _id in self.id2freq}\n",
    "\t\tself.voc_freq_l     = [self.id2freq[_id] for _id in range(len(self.voc2id))]\n",
    "\n",
    "\t\tif not self.p.context: self.p.win_size = 0\n",
    "\n",
    "\t\tself.lib = ctypes.cdll.LoadLibrary('./batchGen.so')\t\t\t# Loads the C++ code for making batches\n",
    "\t\tself.lib.init()\n",
    "\n",
    "\t\t# Creating pointers required for creating batches\n",
    "\t\tself.edges \t= np.zeros(self.p.max_dep_len * self.p.batch_size*3, dtype=np.int32)\n",
    "\t\tself.wrds   = np.zeros(self.p.max_sent_len  * self.p.batch_size,   dtype=np.int32)\n",
    "\t\tself.samp  \t= np.zeros(self.p.max_sent_len  * self.p.batch_size,   dtype=np.int32)\n",
    "\t\tself.negs  \t= np.zeros(self.p.max_sent_len  * self.p.num_neg * self.p.batch_size, dtype=np.int32)\n",
    "\t\tself.wlen  \t= np.zeros(self.p.batch_size, dtype=np.int32)\n",
    "\t\tself.elen  \t= np.zeros(self.p.batch_size, dtype=np.int32)\n",
    "\n",
    "\t\t# Pointer address of above arrays\n",
    "\t\tself.edges_addr = self.edges.ctypes.data_as(ctypes.POINTER(ctypes.c_int))\n",
    "\t\tself.wrds_addr = self.wrds.ctypes.data_as(ctypes.POINTER(ctypes.c_int))\n",
    "\t\tself.negs_addr = self.negs.ctypes.data_as(ctypes.POINTER(ctypes.c_int))\n",
    "\t\tself.samp_addr = self.samp.ctypes.data_as(ctypes.POINTER(ctypes.c_int))\n",
    "\t\tself.wlen_addr = self.wlen.ctypes.data_as(ctypes.POINTER(ctypes.c_int))\n",
    "\t\tself.elen_addr = self.elen.ctypes.data_as(ctypes.POINTER(ctypes.c_int))\n",
    "\n",
    "\tdef add_placeholders(self):\n",
    "\t\tself.sent_wrds \t\t= tf.placeholder(tf.int32,     \tshape=[self.p.batch_size, None],     \t \t     \t name='sent_wrds')\n",
    "\t\tself.sent_mask \t\t= tf.placeholder(tf.float32,   \tshape=[self.p.batch_size, None],     \t \t     \t name='sent_mask')\n",
    "\t\tself.neg_wrds   \t= tf.placeholder(tf.int32,      shape=[self.p.batch_size, None, self.p.num_neg], \t name='neg_wrds')\n",
    "\t\tself.adj_mat   \t\t= tf.placeholder(tf.bool,      \tshape=[self.num_deLabel, self.p.batch_size, None, None], name='adj_ind')\n",
    "\t\tself.num_words\t\t= tf.placeholder(tf.int32,   \tshape=[self.p.batch_size],         \t\t\t name='num_words')\n",
    "\t\tself.seq_len   \t\t= tf.placeholder(tf.int32,   \tshape=(), \t\t   \t\t\t\t name='seq_len')\n",
    "\n",
    "\tdef get_adj(self, batch, seq_len):\n",
    "\t\tnum_edges = np.sum(batch['elen'])\n",
    "\t\tb_ind     = np.expand_dims(np.repeat(np.arange(self.p.batch_size), batch['elen']), axis=1)\n",
    "\t\te_ind     = np.reshape(batch['edges'], [-1, 3])[:num_edges]\n",
    "\n",
    "\t\tadj_ind   = np.concatenate([b_ind, e_ind], axis=1)\n",
    "\t\tadj_ind   = adj_ind[:, [3,0,1,2]]\n",
    "\t\tadj_data  = np.ones(num_edges, dtype=np.float32)\n",
    "\n",
    "\t\treturn COO(adj_ind.T, adj_data, shape=(self.num_deLabel, self.p.batch_size, seq_len, seq_len)).todense()\n",
    "\n",
    "\tdef pad_data(self, data, dlen, sub_sample=[]):\n",
    "\t\tmax_len   = np.max(dlen)\n",
    "\t\tdata_pad  = np.zeros([len(dlen), max_len], dtype=np.int32)\n",
    "\t\tdata_mask = np.zeros([len(dlen), max_len], dtype=np.float32)\n",
    "\n",
    "\t\toffset = 0\n",
    "\t\tfor i in range(len(dlen)):\n",
    "\t\t\tdata_pad [i, :dlen[i]] = data[offset: offset + dlen[i]]\n",
    "\t\t\tdata_mask[i, :dlen[i]] = 1\n",
    "\t\t\tif len(sub_sample) != 0:\n",
    "\t\t\t\tdata_mask[i, :dlen[i]] *= sub_sample[offset: offset + dlen[i]]\n",
    "\t\t\toffset += dlen[i]\n",
    "\n",
    "\t\treturn data_pad, data_mask, max_len\n",
    "\n",
    "\n",
    "\tdef create_feed_dict(self, batch):\n",
    "\t\tfeed_dict = {}\n",
    "\t\twrds_pad, wrds_mask, seq_len \t= self.pad_data(batch['wrds'], batch['wlen'], sub_sample=batch['sample'])\n",
    "\t\tfeed_dict[self.sent_wrds] \t= wrds_pad\n",
    "\t\tfeed_dict[self.sent_mask] \t= wrds_mask\n",
    "\t\tfeed_dict[self.seq_len]   \t= seq_len\n",
    "\t\tfeed_dict[self.adj_mat]   \t= self.get_adj(batch, seq_len)\n",
    "\t\treturn feed_dict\n",
    "\n",
    "\tdef aggregate(self, inp, adj_mat):\n",
    "\t\treturn tf.matmul(tf.cast(adj_mat, tf.float32), inp)\n",
    "\n",
    "\tdef gcnLayer(self, gcn_in, in_dim, gcn_dim, batch_size, max_nodes, max_labels, adj_mat, w_gating=True, num_layers=1, name=\"GCN\"):\n",
    "\t\tout = []\n",
    "\t\tout.append(gcn_in)\n",
    "\n",
    "\t\tfor layer in range(num_layers):\n",
    "\t\t\tgcn_in    = out[-1]\n",
    "\t\t\tif len(out) > 1: in_dim = gcn_dim \t\t# After first iteration the in_dim = gcn_dim\n",
    "\n",
    "\t\t\twith tf.name_scope('%s-%d' % (name,layer)):\n",
    "\n",
    "\t\t\t\tif layer > 0 and self.p.loop:\t\t\t\t\n",
    "\t\t\t\t\twith tf.variable_scope('Loop-name-%s_layer-%d' % (name, layer)) as scope:\n",
    "\t\t\t\t\t\tw_loop  = tf.get_variable('w_loop',  [in_dim, gcn_dim], initializer=tf.contrib.layers.xavier_initializer(), regularizer=self.regularizer)\n",
    "\t\t\t\t\t\tw_gloop = tf.get_variable('w_gloop', [in_dim, 1],       initializer=tf.contrib.layers.xavier_initializer(), regularizer=self.regularizer)\n",
    "\n",
    "\t\t\t\t\t\tinp_loop  = tf.tensordot(gcn_in, w_loop,  axes=[2,0])\n",
    "\t\t\t\t\t\tif self.p.dropout != 1.0: inp_loop  = tf.nn.dropout(inp_loop, keep_prob=self.p.dropout)\n",
    "\n",
    "\t\t\t\t\t\tif w_gating:\n",
    "\t\t\t\t\t\t\tloop_act = tf.tensordot(gcn_in, tf.sigmoid(w_gloop), axes=[2,0])\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tloop_act = inp_loop\n",
    "\n",
    "\t\t\t\t\tact_sum = loop_act\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tact_sum = tf.zeros([batch_size, max_nodes, gcn_dim])\n",
    "\t\t\t\t\n",
    "\n",
    "\t\t\t\tfor lbl in range(max_labels):\n",
    "\n",
    "\t\t\t\t\twith tf.variable_scope('label-%d_name-%s_layer-%d' % (lbl, name, layer)) as scope:\n",
    "\n",
    "\t\t\t\t\t\tw_in   = tf.get_variable('w_in',  \t[in_dim, gcn_dim], \tinitializer=tf.contrib.layers.xavier_initializer(), \t\tregularizer=self.regularizer)\n",
    "\t\t\t\t\t\tw_out  = tf.get_variable('w_out', \t[in_dim, gcn_dim], \tinitializer=tf.contrib.layers.xavier_initializer(), \t\tregularizer=self.regularizer)\n",
    "\t\t\t\t\t\tb_in   = tf.get_variable('b_in',   \t[1,      gcn_dim],\tinitializer=tf.constant_initializer(0.0),\t\t\tregularizer=self.regularizer)\n",
    "\t\t\t\t\t\tb_out  = tf.get_variable('b_out',  \t[1,      gcn_dim],\tinitializer=tf.constant_initializer(0.0),\t\t\tregularizer=self.regularizer)\n",
    "\n",
    "\t\t\t\t\t\tif w_gating:\n",
    "\t\t\t\t\t\t\tw_gin   = tf.get_variable('w_gin',  [in_dim, 1], \tinitializer=tf.contrib.layers.xavier_initializer(), \tregularizer=self.regularizer)\n",
    "\t\t\t\t\t\t\tb_gin   = tf.get_variable('b_gin',  [1], \t\tinitializer=tf.constant_initializer(0.0),\t\tregularizer=self.regularizer)\n",
    "\t\t\t\t\t\t\tw_gout  = tf.get_variable('w_gout', [in_dim, 1], \tinitializer=tf.contrib.layers.xavier_initializer(), \tregularizer=self.regularizer)\n",
    "\t\t\t\t\t\t\tb_gout  = tf.get_variable('b_gout', [1], \t\tinitializer=tf.constant_initializer(0.0),\t\tregularizer=self.regularizer)\n",
    "\n",
    "\n",
    "\t\t\t\t\twith tf.name_scope('in_arcs-%s_name-%s_layer-%d' % (lbl, name, layer)):\n",
    "\n",
    "\t\t\t\t\t\tinp_in     = tf.tensordot(gcn_in, w_in, axes=[2,0]) + tf.expand_dims(b_in, axis=0)\n",
    "\t\t\t\t\t\tadj_matrix = tf.transpose(adj_mat[lbl], [0,2,1])\n",
    "\n",
    "\t\t\t\t\t\tif self.p.dropout != 1.0: \n",
    "\t\t\t\t\t\t\tinp_in = tf.nn.dropout(inp_in, keep_prob=self.p.dropout)\n",
    "\n",
    "\t\t\t\t\t\tif w_gating:\n",
    "\t\t\t\t\t\t\tinp_gin = tf.tensordot(gcn_in, w_gin, axes=[2,0]) + tf.expand_dims(b_gin, axis=0)\n",
    "\t\t\t\t\t\t\tinp_in  = inp_in * tf.sigmoid(inp_gin)\n",
    "\t\t\t\t\t\t\tin_act  = self.aggregate(inp_in, adj_matrix)\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tin_act = self.aggregate(inp_in, adj_matrix)\n",
    "\t\t\t\t\t\t\t\n",
    "\n",
    "\t\t\t\t\twith tf.name_scope('out_arcs-%s_name-%s_layer-%d' % (lbl, name, layer)):\n",
    "\t\t\t\t\t\tinp_out    = tf.tensordot(gcn_in, w_out, axes=[2,0]) + tf.expand_dims(b_out, axis=0)\n",
    "\t\t\t\t\t\tadj_matrix = adj_mat[lbl]\n",
    "\n",
    "\t\t\t\t\t\tif self.p.dropout != 1.0: \n",
    "\t\t\t\t\t\t\tinp_out = tf.nn.dropout(inp_out, keep_prob=self.p.dropout)\n",
    "\n",
    "\t\t\t\t\t\tif w_gating:\n",
    "\t\t\t\t\t\t\tinp_gout = tf.tensordot(gcn_in, w_gout, axes=[2,0]) + tf.expand_dims(b_gout, axis=0)\n",
    "\t\t\t\t\t\t\tinp_out  = inp_out * tf.sigmoid(inp_gout)\n",
    "\t\t\t\t\t\t\tout_act  = self.aggregate(inp_out, adj_matrix)\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tout_act = self.aggregate(inp_out, adj_matrix)\n",
    "\n",
    "\n",
    "\t\t\t\t\tact_sum += in_act + out_act\n",
    "\n",
    "\t\t\t\tgcn_out = tf.nn.relu(act_sum) if layer != num_layers-1 else act_sum\n",
    "\n",
    "\n",
    "\t\t\t\tout.append(gcn_out)\n",
    "\t\treturn out\n",
    "\n",
    "\tdef add_model(self):\n",
    "\t\twith tf.variable_scope('Embed_mat'):\n",
    "\n",
    "\t\t\t# when target embeddings for initialization is assigned\n",
    "\t\t\tif self.p.embed_loc: \n",
    "\t\t\t\tembed_init\t\t= getEmbeddings(self.p.embed_loc, [self.id2voc[i] for i in range(len(self.voc2id))], self.p.embed_dim)\n",
    "\t\t\t\t_wrd_embed \t    = tf.get_variable('embed_matrix', \\\n",
    "\t\t\t\t\t\t\tinitializer=embed_init, regularizer=self.regularizer)\n",
    "\t\t\telse:\n",
    "\t\t\t\tembed_init\t\t= tf.contrib.layers.xavier_initializer()\n",
    "\t\t\t\t_wrd_embed \t    = tf.get_variable('embed_matrix',   [self.vocab_size,  self.p.embed_dim], \\\n",
    "\t\t\t\t\t\t\tinitializer=embed_init, regularizer=self.regularizer)\n",
    "\n",
    "\t\t\twrd_pad             = tf.Variable(tf.zeros([1, self.p.embed_dim]), trainable=False)\n",
    "\t\t\tself.embed_matrix   = tf.concat([_wrd_embed, wrd_pad], axis=0)\n",
    "\n",
    "\t\t\t_context_matrix     = tf.get_variable('context_matrix', [self.vocab_size,  self.p.embed_dim], \\\n",
    "\t\t\t\t\t\t\tinitializer=tf.contrib.layers.xavier_initializer(), regularizer=self.regularizer)\n",
    "\t\t\tself.context_matrix = tf.concat([_context_matrix, wrd_pad], axis=0)\n",
    "\t\t\tself.context_bias   = tf.get_variable('context_bias', \t[self.vocab_size+1], \t\t      \\\n",
    "\t\t\t\t\t\t\tinitializer=tf.constant_initializer(0.0),           regularizer=self.regularizer)\n",
    "\n",
    "\t\tembed   = tf.nn.embedding_lookup(self.embed_matrix, self.sent_wrds)\n",
    "\t\t\n",
    "\t\tgcn_in     = embed\n",
    "\t\tgcn_in_dim = self.p.embed_dim\n",
    "\n",
    "\t\tgcn_out = self.gcnLayer(gcn_in \t\t= gcn_in, \t\tin_dim \t   = gcn_in_dim, \t\tgcn_dim    = self.p.embed_dim,\n",
    "\t\t\t\t\tbatch_size \t= self.p.batch_size, \tmax_nodes  = self.seq_len, \t\tmax_labels = self.num_deLabel,\n",
    "\t\t\t\t\tadj_mat \t= self.adj_mat, \tnum_layers = self.p.gcn_layer, \tname = \"GCN\")\n",
    "\t\tnn_out = gcn_out[-1]\n",
    "\t\treturn nn_out\n",
    "\n",
    "\tdef add_loss_op(self, nn_out):\n",
    "\t\ttarget_words = tf.reshape(self.sent_wrds, [-1, 1])\n",
    "\n",
    "\t\tneg_ids, _, _ = tf.nn.fixed_unigram_candidate_sampler(\n",
    "\t\t\ttrue_classes\t= tf.cast(target_words, tf.int64),\n",
    "\t\t\tnum_true\t= 1,\n",
    "\t\t\tnum_sampled\t= self.p.num_neg * self.p.batch_size,\n",
    "\t\t\tunique\t\t= True,\n",
    "\t\t\tdistortion\t= 0.75,\n",
    "\t\t\trange_max\t= self.vocab_size,\n",
    "\t\t\tunigrams\t= self.voc_freq_l\n",
    "\t\t)\n",
    "\t\tneg_ids = tf.cast(neg_ids, dtype=tf.int32)\n",
    "\t\tneg_ids = tf.reshape(neg_ids, [self.p.batch_size, self.p.num_neg])\n",
    "\t\tneg_ids = tf.reshape(tf.tile(neg_ids, [1, self.seq_len]), [self.p.batch_size, self.seq_len, self.p.num_neg])\n",
    "\t\t\n",
    "\t\ttarget_ind   = tf.concat([\n",
    "\t\t\t\ttf.expand_dims(self.sent_wrds, axis=2),\n",
    "\t\t\t\tneg_ids\n",
    "\t\t    \t], axis=2)\n",
    "\n",
    "\t\ttarget_labels = tf.concat([\n",
    "\t\t\t\t\ttf.ones( [self.p.batch_size, self.seq_len, 1], dtype=tf.float32), \n",
    "\t\t\t\t\ttf.zeros([self.p.batch_size, self.seq_len, self.p.num_neg], dtype=tf.float32)], \n",
    "\t\t\t\taxis=2)\n",
    "\t\ttarget_embed  = tf.nn.embedding_lookup(self.context_matrix, target_ind)\n",
    "\t\tpred\t      = tf.reduce_sum(tf.expand_dims(nn_out, axis=2) * target_embed, axis=3)\n",
    "\t\ttarget_labels = tf.reshape(target_labels, [self.p.batch_size * self.seq_len, -1])\n",
    "\t\tpred \t      = tf.reshape(pred, [self.p.batch_size * self.seq_len, -1])\n",
    "\t\ttotal_loss    = tf.nn.softmax_cross_entropy_with_logits_v2(labels=target_labels, logits=pred)\n",
    "\n",
    "\t\tmasked_loss   = total_loss * tf.reshape(self.sent_mask, [-1])\n",
    "\t\tloss \t      = tf.reduce_sum(masked_loss) / tf.reduce_sum(self.sent_mask)\n",
    "\n",
    "\t\tif self.regularizer != None:\n",
    "\t\t\tloss += tf.contrib.layers.apply_regularization(self.regularizer, tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n",
    "\n",
    "\t\treturn loss\n",
    "\n",
    "\tdef add_optimizer(self, loss, isAdam=True):\n",
    "\t\twith tf.name_scope('Optimizer'):\n",
    "\t\t\tif isAdam:  optimizer = tf.train.AdamOptimizer(self.p.lr)\n",
    "\t\t\telse:       optimizer = tf.train.GradientDescentOptimizer(self.p.lr)\n",
    "\t\t\ttrain_op  = optimizer.minimize(loss)\n",
    "\n",
    "\t\treturn train_op\n",
    "\n",
    "\tdef __init__(self, params):\n",
    "\t\tself.p = params\n",
    "\n",
    "\t\tif not os.path.isdir(self.p.log_dir): os.system('mkdir {}'.format(self.p.log_dir))\n",
    "\t\tif not os.path.isdir(self.p.emb_dir): os.system('mkdir {}'.format(self.p.emb_dir))\n",
    "\n",
    "\t\tself.logger = get_logger(self.p.name, self.p.log_dir, self.p.config_dir)\n",
    "\n",
    "\n",
    "\t\tself.logger.info(vars(self.p)); pprint(vars(self.p))\n",
    "\t\tself.p.batch_size = self.p.batch_size\n",
    "\n",
    "\t\tif self.p.l2 == 0.0:    self.regularizer = None\n",
    "\t\telse:           \tself.regularizer = tf.contrib.layers.l2_regularizer(scale=self.p.l2)\n",
    "\n",
    "\t\tself.load_data()\n",
    "\t\tself.add_placeholders()\n",
    "\n",
    "\t\tnn_out    = self.add_model()\n",
    "\t\tself.loss = self.add_loss_op(nn_out)\n",
    "\n",
    "\t\tif self.p.opt == 'adam': self.train_op = self.add_optimizer(self.loss)\n",
    "\t\telse:            self.train_op = self.add_optimizer(self.loss, isAdam=False)\n",
    "\n",
    "\t\tself.merged_summ = tf.summary.merge_all()\n",
    "\n",
    "\tdef checkpoint(self, epoch, sess):\n",
    "\n",
    "\t\tembed_matrix, context_matrix \t= sess.run([self.embed_matrix, self.context_matrix])\n",
    "\t\tvoc2vec \t= {wrd: embed_matrix[wid] for wrd, wid in self.voc2id.items()}\n",
    "\t\tembedding \t= Embedding.from_dict(voc2vec)\n",
    "\t\tresults\t\t= evaluate_on_all(embedding)\n",
    "\t\tresults \t= {key: round(val[0], 4) for key, val in results.items()}\n",
    "\t\tcurr_int \t= np.mean(list(results.values()))\n",
    "\t\tself.logger.info('Current Score: {}'.format(curr_int))\n",
    "\n",
    "\t\tif curr_int > self.best_int_avg:\n",
    "\t\t\tself.logger.info(\"Saving embedding matrix\")\n",
    "\t\t\tf = open('{}/{}'.format(self.p.emb_dir, self.p.name), 'w')\n",
    "\t\t\tfor id, wrd in self.id2voc.items():\n",
    "\t\t\t\tf.write('{} {}\\n'.format(wrd, ' '.join([str(round(v, 6)) for v in embed_matrix[id].tolist()])))\n",
    "\n",
    "\t\t\tself.saver.save(sess=sess, save_path=self.save_path)\n",
    "\t\t\tself.best_int_avg = curr_int\n",
    "\n",
    "\tdef run_epoch(self, sess, epoch, shuffle=True):\n",
    "\t\tlosses = []\n",
    "\t\tcnt = 0\n",
    "\n",
    "\t\tst = time.time()\n",
    "\t\tfor step, batch in enumerate(self.getBatches(shuffle)):\n",
    "\t\t\tfeed    = self.create_feed_dict(batch)\n",
    "\t\t\tloss, _ = sess.run([self.loss, self.train_op], feed_dict=feed)\n",
    "\t\t\tlosses.append(loss)\n",
    "\t\t\tcnt += self.p.batch_size\n",
    "\n",
    "\t\t\tif (step+1) % 10 == 0:\n",
    "\t\t\t\tself.logger.info('E:{} (Sents: {}/{} [{}]): Train Loss \\t{:.5}\\t{}\\t{:.5}'.format(epoch, cnt, self.p.total_sents, round(cnt/self.p.total_sents * 100 , 1), np.mean(losses), self.p.name, self.best_int_avg))\n",
    "\t\t\t\ten = time.time()\n",
    "\t\t\t\tif (en-st) >= (3600):\n",
    "\t\t\t\t\tself.logger.info(\"One more hour is over\")\n",
    "\t\t\t\t\tself.checkpoint(epoch, sess)\n",
    "\t\t\t\t\tst = time.time()\n",
    "\n",
    "\t\treturn np.mean(losses)\n",
    "\n",
    "\tdef fit(self, sess):\n",
    "\t\tself.saver       = tf.train.Saver()\n",
    "\t\tsave_dir  \t = 'checkpoints/' + self.p.name + '/'\n",
    "\t\tif not os.path.exists(save_dir): os.makedirs(save_dir)\n",
    "\t\tself.save_path   = os.path.join(save_dir, 'best_int_avg')\n",
    "\n",
    "\t\tself.best_int_avg  = 0.0\n",
    "\n",
    "\t\tif self.p.restore:\n",
    "\t\t\tself.saver.restore(sess, self.save_path)\n",
    "\n",
    "\t\tfor epoch in range(self.p.max_epochs):\n",
    "\t\t\tself.logger.info('Epoch: {}'.format(epoch))\n",
    "\t\t\ttrain_loss = self.run_epoch(sess, epoch)\n",
    "\n",
    "\t\t\tself.checkpoint(epoch, sess)\n",
    "\t\t\tself.logger.info('[Epoch {}]: Training Loss: {:.5}, Best Loss: {:.5}\\n'.format(epoch, train_loss,  self.best_int_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25583a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__== \"__main__\":\n",
    "\n",
    "\tparser = argparse.ArgumentParser(description='WORD GCN')\n",
    "\n",
    "\tparser.add_argument('-gpu',      dest=\"gpu\",            default='0',                \thelp='GPU to use')\n",
    "\tparser.add_argument('-name',     dest=\"name\",           default='test_run',             help='Name of the run')\n",
    "\tparser.add_argument('-embed',    dest=\"embed_loc\",      default=None,         \t\thelp='Embedding for initialization')\n",
    "\tparser.add_argument('-embed_dim',dest=\"embed_dim\",      default=300,      type=int,     help='Embedding Dimension')\n",
    "\tparser.add_argument('-total',    dest=\"total_sents\",    default=56974869, type=int,     help='Total number of sentences in file')\n",
    "\tparser.add_argument('-lr',       dest=\"lr\",             default=0.001,    type=float,   help='Learning rate')\n",
    "\tparser.add_argument('-batch',    dest=\"batch_size\",     default=128,      type=int,     help='Batch size')\n",
    "\tparser.add_argument('-epoch',    dest=\"max_epochs\",     default=50,       type=int,     help='Max epochs')\n",
    "\tparser.add_argument('-l2',       dest=\"l2\",             default=0.00001,  type=float,   help='L2 regularization')\n",
    "\tparser.add_argument('-seed',     dest=\"seed\",           default=1234,     type=int,     help='Seed for randomization')\n",
    "\tparser.add_argument('-sample',\t dest=\"sample\",\t  \tdefault=1e-4,     type=float,   help='Subsampling parameter')\n",
    "\tparser.add_argument('-neg',      dest=\"num_neg\",    \tdefault=100,      type=int,     help='Number of negative samples')\n",
    "\tparser.add_argument('-side_int', dest=\"side_int\",    \tdefault=10000,    type=int,     help='Number of negative samples')\n",
    "\tparser.add_argument('-gcn_layer',dest=\"gcn_layer\",      default=1,        type=int,     help='Number of layers in GCN over dependency tree')\n",
    "\tparser.add_argument('-drop',     dest=\"dropout\",        default=1.0,      type=float,   help='Dropout for full connected layer (Keep probability')\n",
    "\tparser.add_argument('-opt',      dest=\"opt\",            default='adam',             \thelp='Optimizer to use for training')\n",
    "\tparser.add_argument('-dump',  \t dest=\"onlyDump\",       action='store_true',        \thelp='Dump context and embed matrix')\n",
    "\tparser.add_argument('-context',  dest=\"context\",        action='store_true',        \thelp='Include sequential context edges (default: False)')\n",
    "\tparser.add_argument('-restore',  dest=\"restore\",        action='store_true',        \thelp='Restore from the previous best saved model')\n",
    "\tparser.add_argument('-embdir',   dest=\"emb_dir\",        default='./embeddings/',       \thelp='Directory for storing learned embeddings')\n",
    "\tparser.add_argument('-logdir',   dest=\"log_dir\",        default='./log/',       \thelp='Log directory')\n",
    "\tparser.add_argument('-config',   dest=\"config_dir\",     default='./config/',        \thelp='Config directory')\n",
    "\n",
    "\t# Added these two arguments to enable others to personalize the training set. Otherwise, the programme may suffer from memory overflow easily.\n",
    "\t# It is suggested that the -maxlen be set no larger than 100.\n",
    "\tparser.add_argument('-maxsentlen',dest=\"max_sent_len\",\tdefault=50, \t  type=int,\thelp='Max length of the sentences in data.txt (default: 40)')\n",
    "\tparser.add_argument('-maxdeplen', dest=\"max_dep_len\", \tdefault=800,\t  type=int,\thelp='Max length of the dependency relations in data.txt (default: 800)')\n",
    "\n",
    "\targs = parser.parse_args()\n",
    "\n",
    "\tif not args.restore: args.name = args.name + '_' + time.strftime(\"%d_%m_%Y\") + '_' + time.strftime(\"%H:%M:%S\")\n",
    "\n",
    "\ttf.set_random_seed(args.seed)\n",
    "\trandom.seed(args.seed)\n",
    "\tnp.random.seed(args.seed)\n",
    "\tset_gpu(args.gpu)\n",
    "\n",
    "\tmodel = SynGCN(args)\n",
    "\n",
    "\tconfig = tf.ConfigProto()\n",
    "\tconfig.gpu_options.allow_growth=True\n",
    "\twith tf.Session(config=config) as sess:\n",
    "\t\tsess.run(tf.global_variables_initializer())\n",
    "\t\tmodel.fit(sess)\n",
    "\n",
    "\tprint('Model Trained Successfully!!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
